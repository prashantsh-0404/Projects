{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww37900\viewh21300\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\
\
\
\
val rawWeatherDF = sqlContext.table("weather")\
\
\
val header = rawWeatherDF.first()\
\
\
\
val noHeaderWeatherDF = rawWeatherDF.filter(row => row != header)\
\
\
val toFahrenheit = udf \{(c: Double) => c * 9 / 5 + 32\}\
\
\
\
val weatherDF=noHeaderWeatherDF.withColumn("new_tmin",toFahrenheit(noHeaderWeatherDF("tmin"))).withColumn("new_tmax",toFahrenheit(noHeaderWeatherDF("tmax"))).drop("tmax").drop("tmin").withColumnRenamed("new_tmax","tmax").withColumnRenamed("new_tmin","tmin")\
\
\
val jdbcURL = "jdbc:redshift://redshift-cluster-1.cs4itg7hgbxx.us-east-1.redshift.amazonaws.com:5439/dev?user=shishop&password=Admin0404"\
\
val s3TempDir = "s3://AKIAWU724ZEBG4GKJOUW:WTD8k+Ip3bmyq%2FEGfrRAn2so7PX2eOzD6vvMYLGK@projects-1992"\
\
\
val flightsQuery = """select ID, DAY_OF_MONTH, DAY_OF_WEEK, FL_DATE, f_days_from_holiday(year, month, day_of_month) as DAYS_TO_HOLIDAY, UNIQUE_CARRIER, FL_NUM, substring(DEP_TIME, 1, 2) as DEP_HOUR, cast(DEP_DEL15 as smallint),cast(AIR_TIME as integer), cast(FLIGHTS as smallint), cast(DISTANCE as smallint) from ord_flights where origin='ORD' and cancelled = 0"""\
\
\
val flightsDF=sqlContext.read.format("com.databricks.spark.redshift").option("url",jdbcURL).option("tempdir",s3TempDir).option("query",flightsQuery).load()\
\
val joinedDF = flightsDF.join(weatherDF, flightsDF("fl_date") === weatherDF("dt"))\
\
\
joinedDF.write.format("com.databricks.spark.redshift").option("url", jdbcURL).option("dbtable", "ord_flights2").option("aws_iam_role", "arn:aws:iam::457403320578:role/redshift-EXT").option("tempdir", s3TempDir).mode("error").save()\
\
//joinedDF.write.format("com.databricks.spark.redshift").option("temporary_aws_access_key_id", awsAccessKey).option("temporary_aws_secret_access_key", awsSecretKey).option("url", jdbcURL).option("dbtable", "ord_flights2").option("aws_iam_role", "arn:aws:iam::457403320578:role/redshift-EXT").option("tempdir", s3TempDir).mode("error").save()}